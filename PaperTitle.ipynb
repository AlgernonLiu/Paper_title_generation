{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example testing original model:\n",
    "# from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "# #!pip install sentencepiece\n",
    "\n",
    "# #import sentencepiece\n",
    "\n",
    "# model = MT5ForConditionalGeneration.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "\n",
    "# chunk = \"\"\"å¤ªå¹³å¤©å›½å é¢†åŒºè¡—å¸‚æ²¡æœ‰åˆ»å­—é“ºï¼Œæ‰€æœ‰åˆ»å­—åŒ äººéƒ½ç¼–å…¥é•Œåˆ»è¥ï¼Œâ€œæœå‹‹è©¹è®°â€ä¸€å°åº”ä¸ºå¤ªå¹³å¤©å›½é•Œåˆ»è¥æ‰€å‡ºã€‚ä½†å®ƒä¸å±äºå¤ªå¹³å¤©å›½ç¤¼éƒ¨ç»Ÿä¸€åˆ¶å‘çš„å°ç« ã€‚å¤ªå¹³å¤©å›½ç§äººä¾¿ç« å®ç‰©ä»æ— å‘ç°ï¼Œâ€œæœå‹‹è©¹è®°â€ä¸€å°çš„å‘ç°ï¼Œå¼¥è¶³çè´µï¼Œå®ƒä¸ºç ”ç©¶å¤ªå¹³å¤©å›½ç”¨å°æƒ…å†µåŠåˆ¶åº¦æä¾›äº†ç¬¬ä¸€æ‰‹é‡è¦å®ç‰©èµ„æ–™ã€‚\n",
    "# \"\"\"\n",
    "# inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors='pt', max_length=4096, truncation=True)\n",
    "# summary_ids = model.generate(inputs, max_length=512, num_beams=4, length_penalty=0.2, no_repeat_ngram_size=2)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We use the cleaned data from CSL dataset, and load the evaluation method, Rouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4140588/3967303350.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "raw_datasets = load_from_disk(\"./Paper\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each piece of data containing document(Paper abstract), summary(Paper title) and id, like the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“GNå’ŒRNåœ¨å›ºè‰²æµ´ä¸­å‡èšæ€§å°ã€éª¤æŸ“æ€§å°ã€åŒ€æŸ“æ€§å¥½,ä¸”å¸å°½ç‡å’Œå›ºè‰²ç‡é«˜ã€æå‡æ€§å’Œé‡ç°æ€§å¥½,è¾ƒå¥½åœ°å…‹æœäº†å¸¸ç”¨å•ä¹™çƒ¯ç œå‹æ´»æ€§è‰³è“(C.I.B-19)çš„æ€§èƒ½ç¼ºé™·.è¯¥æŸ“æ–™æœ€é€‚åˆ70â„ƒæŸ“è‰²,ä¸å«©é»„Y-160æˆ–ç¿ è“B-21é…ä¼æ‹¼æŸ“è‰³ç»¿è‰²æˆ–è‰³è“è‰²,å¯ä»¥å¤§å¹…æé«˜æŸ“è‰²ä¸€ç­‰å“ç‡.',\n",
       " 'summary': 'åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„åº”ç”¨æ€§èƒ½',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tokenize\n",
    "\n",
    "Before we input the data, we should use tokenizer to transform the natural language to vector. We use the tokenizer from pretrained model MT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"heack/HeackMT5-ZhSum100k\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure all the data are computed in the same device, we assign the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.45 GiB of which 7.69 MiB is free. Process 4002836 has 37.81 GiB memory in use. Process 4002839 has 8.19 GiB memory in use. Including non-PyTorch memory, this process has 1.42 GiB memory in use. Of the allocated memory 1.23 GiB is allocated by PyTorch, and 38.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.45 GiB of which 7.69 MiB is free. Process 4002836 has 37.81 GiB memory in use. Process 4002839 has 8.19 GiB memory in use. Including non-PyTorch memory, this process has 1.42 GiB memory in use. Of the allocated memory 1.23 GiB is allocated by PyTorch, and 38.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before tokenizing, we also need to decide the max input and output length. Given that the title and abstract are not to long, I decide to use small length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 32\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4544c9a58cd49668af03e736cde85b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b179f388b4839b1e44f493ac31cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cb9b7323a34c50b7ca74b1062d619d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We can now start the training. First we import the necessary packages and set the hyperparameter within the training. \n",
    "You can set the batch_size and num_train_epochs here, I set the epoch in 1, 2, and 5 for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "batch_size = 49\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    logging_dir='./logs',  # Set the logging directory\n",
    "    logging_steps=100,  # Log every 100 steps\n",
    "    disable_tqdm=False,  # Ensure the progress bar and logging are enabled\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a data collator to input the raw data into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForSeq2Seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[43mDataCollatorForSeq2Seq\u001b[49m(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForSeq2Seq' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we set the evaluation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    print(\"reference summary: \", decoded_labels[0])\n",
    "    print(\"generated summary: \", decoded_preds[0])\n",
    "    \n",
    "    predictions_tensor = torch.tensor(predictions)\n",
    "    generated_preds = []\n",
    "    for input_ids in predictions_tensor:\n",
    "        input_ids = input_ids.to(device)\n",
    "    \n",
    "        generated_summary_ids = model.generate(\n",
    "            input_ids.unsqueeze(0),  \n",
    "            max_length=max_target_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=0.4,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
    "        generated_preds.append(generated_summary)\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final step before our training: send all the parameter into trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each epoch the trainer will evaluate the model and output the score and I also set the trainer to output the first generation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 58:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.642700</td>\n",
       "      <td>1.282349</td>\n",
       "      <td>4.796700</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.793300</td>\n",
       "      <td>4.741700</td>\n",
       "      <td>12.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>1.263666</td>\n",
       "      <td>5.498400</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>5.508200</td>\n",
       "      <td>5.508200</td>\n",
       "      <td>14.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>1.364481</td>\n",
       "      <td>6.287900</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>6.299800</td>\n",
       "      <td>6.222600</td>\n",
       "      <td>13.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>1.503404</td>\n",
       "      <td>6.267300</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>6.297300</td>\n",
       "      <td>6.237700</td>\n",
       "      <td>14.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>1.731158</td>\n",
       "      <td>5.994500</td>\n",
       "      <td>1.083300</td>\n",
       "      <td>6.022900</td>\n",
       "      <td>5.958700</td>\n",
       "      <td>14.630000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  ä¸€ç§åŸºäºå¹³æ–¹å’Œä¼˜åŒ–çš„é£è¡Œå™¨å¤§è§’åº¦æœºåŠ¨é•‡å®šæ§åˆ¶å™¨è®¾è®¡æ–¹æ³•\n",
      "generated summary:  åŸºäºå¹³æ–¹/Mçš„é£è¡Œå™¨å§¿æ€æ§åˆ¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  ä¸€ç§åŸºäºå¹³æ–¹å’Œä¼˜åŒ–çš„é£è¡Œå™¨å¤§è§’åº¦æœºåŠ¨é•‡å®šæ§åˆ¶å™¨è®¾è®¡æ–¹æ³•\n",
      "generated summary:  åŸºäºå±‚æ¬¡åˆ†æçš„é£è¡Œå™¨çºµå‘æ§åˆ¶\n",
      "reference summary:  ä¸€ç§åŸºäºå¹³æ–¹å’Œä¼˜åŒ–çš„é£è¡Œå™¨å¤§è§’åº¦æœºåŠ¨é•‡å®šæ§åˆ¶å™¨è®¾è®¡æ–¹æ³•\n",
      "generated summary:  åŸºäºä¿®æ­£çš„é£è¡Œå™¨å§¿æ€æ§åˆ¶ç³»ç»Ÿå»ºæ¨¡ä¸ä»¿çœŸ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  ä¸€ç§åŸºäºå¹³æ–¹å’Œä¼˜åŒ–çš„é£è¡Œå™¨å¤§è§’åº¦æœºåŠ¨é•‡å®šæ§åˆ¶å™¨è®¾è®¡æ–¹æ³•\n",
      "generated summary:  åŸºäºä¿®æ­£çš„é£è¡Œå™¨å§¿æ€æ§åˆ¶ç³»ç»Ÿå»ºæ¨¡ä¸ä»¿çœŸ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  ä¸€ç§åŸºäºå¹³æ–¹å’Œä¼˜åŒ–çš„é£è¡Œå™¨å¤§è§’åº¦æœºåŠ¨é•‡å®šæ§åˆ¶å™¨è®¾è®¡æ–¹æ³•\n",
      "generated summary:  åŸºäºä¿®æ­£çš„é£è¡Œå™¨å§¿æ€æ§åˆ¶\n"
     ]
    }
   ],
   "source": [
    "output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 84, 'num_beams': 4, 'length_penalty': 0.6}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"finetune5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "I want to compare the performance between original and finetuned model.\n",
    "\n",
    "First let us see how to evaluate a single model using test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba2739d630c42bfa6b41aa715d3900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„åº”ç”¨æ€§èƒ½\n",
      "generated summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“åœ¨å›ºè‰²æµ´ä¸­å‘æŒ¥é‡è¦ä½œç”¨\n",
      "Evaluation Results for Original Model:\n",
      "{'eval_loss': 1.2633886337280273, 'eval_rouge1': 7.5571, 'eval_rouge2': 1.1767, 'eval_rougeL': 7.5714, 'eval_rougeLsum': 7.6779, 'eval_gen_len': 15.773, 'eval_runtime': 341.6383, 'eval_samples_per_second': 2.927, 'eval_steps_per_second': 0.061}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "batch_size = 48\n",
    "max_target_length = 32\n",
    "\n",
    "# Load the original model and tokenizer\n",
    "original_model = MT5ForConditionalGeneration.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "\n",
    "# Select the training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)\n",
    "\n",
    "# Create training arguments\n",
    "ori_training_args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    ")\n",
    "\n",
    "# Ensure raw_datasets[\"test\"] is not empty\n",
    "if raw_datasets[\"test\"] is None:\n",
    "    raise ValueError(\"The validation dataset is None. Please provide a valid dataset.\")\n",
    "\n",
    "# Create Seq2SeqTrainer\n",
    "ori_trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=ori_training_args,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate the original model\n",
    "results = ori_trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results for Original Model:\")\n",
    "print(results)\n",
    "\n",
    "# # Print some example results\n",
    "\n",
    "# val_dataset = raw_datasets[\"validation\"]\n",
    "# if val_dataset is not None:\n",
    "#     for i in range(5):\n",
    "#         inputs = tokenizer(\"summarize: \" + val_dataset[i][\"document\"], return_tensors='pt', max_length=4096, truncation=True).to(device)\n",
    "#         summary_ids = original_model.generate(inputs.input_ids, max_length=512, num_beams=4, length_penalty=0.2, no_repeat_ngram_size=2)\n",
    "#         generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#         reference_summary = val_dataset[i][\"summary\"]\n",
    "\n",
    "#         print(f\"Reference: {reference_summary}\")\n",
    "#         print(f\"Generated: {generated_summary}\")\n",
    "#         print(\"\\n\")\n",
    "# else:\n",
    "#     raise ValueError(\"The validation dataset is None. Please provide a valid dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similar to the code above, we evaluate the score among original and finetuned models in 1, 2 and 5 epochs at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/xxliu/.conda/envs/myenv/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„åº”ç”¨æ€§èƒ½\n",
      "generated summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“åœ¨å›ºè‰²æµ´ä¸­å‘æŒ¥é‡è¦ä½œç”¨\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„åº”ç”¨æ€§èƒ½\n",
      "generated summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“èŠªæŸ“æ–™çš„æŸ“è‰²\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„åº”ç”¨æ€§èƒ½\n",
      "generated summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„æŸ“è‰²\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“çš„åº”ç”¨æ€§èƒ½\n",
      "generated summary:  åŒå®˜èƒ½å›¢æ´»æ€§è‰³è“é¢†åŠæ—ä¸šåœ¨ç´«èœä¸­çš„åº”ç”¨\n",
      "Evaluation Results for Original Model on Test Set:\n",
      "{'eval_loss': 1.2633886337280273, 'eval_rouge1': 7.5571, 'eval_rouge2': 1.1767, 'eval_rougeL': 7.5714, 'eval_rougeLsum': 7.6779, 'eval_gen_len': 15.773, 'eval_runtime': 431.7017, 'eval_samples_per_second': 2.316, 'eval_steps_per_second': 0.049}\n",
      "Evaluation Results for Fine-tuned Model on Test Set:\n",
      "{'eval_loss': 1.0314708948135376, 'eval_rouge1': 7.4925, 'eval_rouge2': 0.7, 'eval_rougeL': 7.4099, 'eval_rougeLsum': 7.4257, 'eval_gen_len': 14.044, 'eval_runtime': 325.5001, 'eval_samples_per_second': 3.072, 'eval_steps_per_second': 0.065}\n",
      "Evaluation Results for Fine-tuned Model2 on Test Set:\n",
      "{'eval_loss': 1.1142152547836304, 'eval_rouge1': 7.8213, 'eval_rouge2': 0.69, 'eval_rougeL': 7.7348, 'eval_rougeLsum': 7.8231, 'eval_gen_len': 13.999, 'eval_runtime': 432.3147, 'eval_samples_per_second': 2.313, 'eval_steps_per_second': 0.049}\n",
      "Evaluation Results for Fine-tuned Model5 on Test Set:\n",
      "{'eval_loss': 1.747678518295288, 'eval_rouge1': 7.223, 'eval_rouge2': 0.5014, 'eval_rougeL': 7.1389, 'eval_rougeLsum': 7.2078, 'eval_gen_len': 14.389, 'eval_runtime': 375.9517, 'eval_samples_per_second': 2.66, 'eval_steps_per_second': 0.056}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load evaluation metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Load the original model and tokenizer\n",
    "original_model = MT5ForConditionalGeneration.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "\n",
    "# Load fine-tuned models\n",
    "finetuned_model = MT5ForConditionalGeneration.from_pretrained(\"./finetune1\")\n",
    "finetuned_model2 = MT5ForConditionalGeneration.from_pretrained(\"./finetune2\")\n",
    "finetuned_model5 = MT5ForConditionalGeneration.from_pretrained(\"./finetune5\")\n",
    "\n",
    "# Select the training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "finetuned_model.to(device)\n",
    "finetuned_model2.to(device)\n",
    "finetuned_model5.to(device)\n",
    "\n",
    "# Create training arguments\n",
    "testing_args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    logging_dir='./logs',  # Set the logging directory\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    disable_tqdm=False,  # Ensure the progress bar and logging are enabled\n",
    ")\n",
    "\n",
    "# Ensure raw_datasets[\"test\"] is not empty\n",
    "if raw_datasets[\"test\"] is None:\n",
    "    raise ValueError(\"The test dataset is None. Please provide a valid dataset.\")\n",
    "\n",
    "# Create Seq2SeqTrainer instances to evaluate the original model\n",
    "original_trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Create Seq2SeqTrainer instances to evaluate the fine-tuned model\n",
    "finetuned_trainer = Seq2SeqTrainer(\n",
    "    model=finetuned_model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Create Seq2SeqTrainer instances to evaluate the fine-tuned2 model\n",
    "finetuned2_trainer = Seq2SeqTrainer(\n",
    "    model=finetuned_model2,\n",
    "    args=testing_args,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Create Seq2SeqTrainer instances to evaluate the fine-tuned5 model\n",
    "finetuned5_trainer = Seq2SeqTrainer(\n",
    "    model=finetuned_model5,\n",
    "    args=testing_args,\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate the original model\n",
    "original_results = original_trainer.evaluate()\n",
    "\n",
    "# Evaluate the fine-tuned models\n",
    "finetuned_results = finetuned_trainer.evaluate()\n",
    "finetuned2_results = finetuned2_trainer.evaluate()\n",
    "finetuned5_results = finetuned5_trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results for Original Model:\")\n",
    "print(original_results)\n",
    "print(\"Evaluation Results for Fine-tuned Model after 1 epoch:\")\n",
    "print(finetuned_results)\n",
    "print(\"Evaluation Results for Fine-tuned Model after 2 epoch:\")\n",
    "print(finetuned2_results)\n",
    "print(\"Evaluation Results for Fine-tuned Model after 5 epoch:\")\n",
    "print(finetuned5_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
